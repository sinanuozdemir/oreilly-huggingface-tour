{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5820df57-ff2d-4440-a1c6-665ddd18c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_KEY = 'hsdfsddsfgdsf'  # replace with your own token: huggingface.co/settings/tokens\n",
    "GROQ_API_KEY = 'gsk_dfjdkfng'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbc9a2-125f-4afc-a40e-0e55c131c421",
   "metadata": {},
   "source": [
    "# Groq to use Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0122984-c6b3-448f-8611-60b4b3a61c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key= GROQ_API_KEY\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":  \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f9dc2-05c3-48d8-95e6-26d076c16ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5d7566-da6c-4c56-a4ed-71218402fb17",
   "metadata": {},
   "source": [
    "# Using Inference with [Together AI](https://together.ai/) to use DeepSeek R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96dc0e61-3b2e-4465-9037-3bf8b42610dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.30.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub; huggingface_hub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bcafba-f690-4277-93d4-d00f5cc91e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "together_client = InferenceClient(\n",
    "\tprovider=\"together\",\n",
    "\tapi_key=HF_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = together_client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb1ad11-b416-4669-83e8-830c12a7c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the capital of France. That's a straightforward geography question. \n",
      "\n",
      "Hmm, I recall that Paris is definitely the capital - it's one of those basic facts everyone learns in school. But let me double-check mentally: yes, Paris has been the capital since at least the 10th century, with brief exceptions during wars. \n",
      "\n",
      "The user might be a student doing homework, or maybe someone verifying trivia. Since they didn't provide context, I'll keep it simple. No need for extra details unless they follow up. \n",
      "\n",
      "I should mention it's also France's largest city - that adds useful context. And the Seine river reference makes it more vivid. \n",
      "\n",
      "The response should be cheerful but not overly excited - it's basic information after all. A smiley face would make it friendly without being unprofessional. \n",
      "\n",
      "No signs this is a trick question - just giving the clear answer they requested.\n",
      "</think>\n",
      "The capital of France is **Paris**.  \n",
      "\n",
      "Paris is not only the political center of France but also its largest city and a global hub for art, fashion, gastronomy, and culture. It is situated in the north-central part of the country, along the Seine River.  \n",
      "\n",
      "Iconic landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral make Paris one of the world's most visited cities! ðŸ‡«ðŸ‡·\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de93a845-eeed-45f6-b3e2-864f2a862dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='', timeout=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d979a93b-bcf7-47e4-99e9-b917c69c28ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/deepseek-ai/DeepSeek-R1/v1/chat/completions (Request ID: Root=1-68653054-375458496f85d60c1e313a0f;52063302-1e40-4851-86b9-6901bea03f0d)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/deepseek-ai/DeepSeek-R1/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(\n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;66;03m# provider=\"together\",  without together yields error because HF doesn't have an inference endpoint for this model \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \tapi_key\u001b[38;5;241m=\u001b[39mHF_KEY\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:992\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    964\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    984\u001b[0m }\n\u001b[1;32m    985\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m    986\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    987\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    991\u001b[0m )\n\u001b[0;32m--> 992\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/deepseek-ai/DeepSeek-R1/v1/chat/completions (Request ID: Root=1-68653054-375458496f85d60c1e313a0f;52063302-1e40-4851-86b9-6901bea03f0d)"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\t# provider=\"together\",  without together yields error because HF doesn't have an inference endpoint for this model \n",
    "\tapi_key=HF_KEY\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bad13-c182-4c6a-87a1-eac1ac5aa03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3eebbc-23e9-4ef9-a201-4d0850ed4337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. It is the country's largest city and main cultural and commercial center, known for landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n"
     ]
    }
   ],
   "source": [
    "# Using Cohere as an inference provider: https://huggingface.co/blog/inference-providers-cohere\n",
    "\n",
    "cohere_hf_client = InferenceClient(\n",
    "    provider=\"cohere\",\n",
    "    api_key=HF_KEY,\n",
    ")\n",
    "# https://huggingface.co/CohereLabs/c4ai-command-a-03-2025\n",
    "completion = cohere_hf_client.chat.completions.create(\n",
    "    model=\"CohereLabs/c4ai-command-a-03-2025\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0aa91-c4ff-43b2-a237-1bcc3674256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc009d6-ce52-4c76-a57a-f7f11283b419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3c433-7cdd-4b59-b362-444c79a819d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d76409-afb4-4471-bc58-63740c08b71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852acb7f-55e4-4317-aec1-a0866f62c3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c239ab8e-f416-4255-8da7-bbb0d7a614c1",
   "metadata": {},
   "source": [
    "# VQA Using Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c8b89f3-9afb-439e-8df8-ebee2b955203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The Statue of Liberty stands majestically on Liberty Island, with the iconic New York City skyline as its backdrop across the water.', tool_call_id=None, tool_calls=[], refusal=None, audio=None, function_call=None, reasoning_content=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"nebius\",\n",
    "    api_key=HF_KEY,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in one sentence.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6879ad1-290c-44d5-a27d-d9dbeab2fc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
